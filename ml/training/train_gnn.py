"""
Training script for Graph Neural Network (GNN) pet compatibility model
Uses the social graph data generated by generate_graph_data.py
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import torch.nn as nn
import torch.optim as optim
from torch_geometric.data import Data, DataLoader
from torch_geometric.utils import negative_sampling
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import json
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, average_precision_score

from models.gnn_compatibility import GraphAttentionCompatibility

# Configuration
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
DATA_DIR = Path('ml/data')
MODEL_DIR = Path('ml/models/saved')
MODEL_DIR.mkdir(parents=True, exist_ok=True)

# Hyperparameters
BATCH_SIZE = 1  # Full graph training
NUM_EPOCHS = 100
LEARNING_RATE = 0.001
WEIGHT_DECAY = 1e-5
PATIENCE = 15
MIN_DELTA = 0.001

# Model architecture
EMBEDDING_DIM = 64
HIDDEN_DIM = 128
GNN_LAYERS = 3
ATTENTION_HEADS = 4
DROPOUT = 0.2


def load_graph_data():
    """Load the social graph data"""
    print("Loading graph data...")

    # Load pets data
    pets_df = pd.read_csv(DATA_DIR / 'graph_pets.csv')
    edges_df = pd.read_csv(DATA_DIR / 'graph_edges.csv')

    print(f"Loaded {len(pets_df)} pets and {len(edges_df)} edges")

    # Create mappings
    breed_to_idx = {breed: idx for idx, breed in enumerate(pets_df['breed'].unique())}
    temp_to_idx = {temp: idx for idx, temp in enumerate(pets_df['temperament'].unique())}

    # Save mappings for later use
    mappings = {
        'breed_to_idx': breed_to_idx,
        'temp_to_idx': temp_to_idx,
        'idx_to_breed': {v: k for k, v in breed_to_idx.items()},
        'idx_to_temp': {v: k for k, v in temp_to_idx.items()},
    }

    with open(MODEL_DIR / 'feature_mappings.json', 'w') as f:
        json.dump(mappings, f, indent=2)

    return pets_df, edges_df, breed_to_idx, temp_to_idx


def create_graph_dataset(pets_df, edges_df, breed_to_idx, temp_to_idx):
    """Convert dataframes to PyTorch Geometric Data object"""
    print("Creating graph dataset...")

    # Create node features
    num_pets = len(pets_df)

    # Feature tensor: [breed_idx, temp_idx, size_one_hot (3), energy_one_hot (3), age_normalized, weight_normalized]
    features = []

    size_map = {'small': 0, 'medium': 1, 'large': 2}
    energy_map = {'low': 0, 'medium': 1, 'high': 2}

    for _, pet in pets_df.iterrows():
        # Breed and temperament indices
        breed_idx = breed_to_idx[pet['breed']]
        temp_idx = temp_to_idx[pet['temperament']]

        # Size one-hot
        size_onehot = [0, 0, 0]
        size_onehot[size_map[pet['size']]] = 1

        # Energy one-hot
        energy_onehot = [0, 0, 0]
        energy_onehot[energy_map[pet['energy']]] = 1

        # Normalized age and weight
        age_norm = pet['age'] / 15.0  # Normalize by max age
        weight_norm = pet['weight'] / 100.0  # Normalize by max weight

        # Combine features
        feature_vector = [breed_idx, temp_idx] + size_onehot + energy_onehot + [age_norm, weight_norm]
        features.append(feature_vector)

    x = torch.tensor(features, dtype=torch.float)

    # Create edge index from positive edges
    edge_index = torch.tensor([
        edges_df['pet_a'].values,
        edges_df['pet_b'].values
    ], dtype=torch.long)

    # Edge attributes (interaction count as strength)
    edge_attr = torch.tensor(edges_df['interactions_count'].values, dtype=torch.float).unsqueeze(1)

    # Create labels (1 for positive edges)
    edge_labels = torch.ones(edge_index.shape[1], dtype=torch.float)

    # Generate negative edges for training
    neg_edge_index = negative_sampling(
        edge_index=edge_index,
        num_nodes=num_pets,
        num_neg_samples=edge_index.shape[1],  # Same number as positive
    )

    # Combine positive and negative edges
    full_edge_index = torch.cat([edge_index, neg_edge_index], dim=1)
    full_edge_labels = torch.cat([
        edge_labels,
        torch.zeros(neg_edge_index.shape[1], dtype=torch.float)
    ])

    # Create edge attributes for negative edges (set to 0)
    neg_edge_attr = torch.zeros(neg_edge_index.shape[1], 1, dtype=torch.float)
    full_edge_attr = torch.cat([edge_attr, neg_edge_attr], dim=0)

    # Create train/val/test splits for edges
    num_edges = full_edge_index.shape[1]
    indices = np.arange(num_edges)

    train_idx, temp_idx = train_test_split(indices, test_size=0.3, random_state=42)
    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)

    data = Data(
        x=x,
        edge_index=full_edge_index,
        edge_attr=full_edge_attr,
        y=full_edge_labels,
        train_idx=torch.tensor(train_idx, dtype=torch.long),
        val_idx=torch.tensor(val_idx, dtype=torch.long),
        test_idx=torch.tensor(test_idx, dtype=torch.long),
    )

    print(f"Created graph with {data.num_nodes} nodes and {data.edge_index.shape[1]} edges")
    print(f"Train edges: {len(train_idx)}, Val edges: {len(val_idx)}, Test edges: {len(test_idx)}")

    return data


def train_epoch(model, data, optimizer, criterion, train_idx):
    """Train for one epoch"""
    model.train()
    optimizer.zero_grad()

    # Forward pass
    predictions = model(
        data.x.to(DEVICE),
        data.edge_index.to(DEVICE),
        data.edge_attr.to(DEVICE)
    )

    # Calculate loss on training edges only
    loss = criterion(predictions[train_idx], data.y[train_idx].to(DEVICE))

    # Backward pass
    loss.backward()
    optimizer.step()

    return loss.item()


@torch.no_grad()
def evaluate(model, data, criterion, eval_idx, split_name='val'):
    """Evaluate the model"""
    model.eval()

    # Forward pass
    predictions = model(
        data.x.to(DEVICE),
        data.edge_index.to(DEVICE),
        data.edge_attr.to(DEVICE)
    )

    # Calculate loss
    loss = criterion(predictions[eval_idx], data.y[eval_idx].to(DEVICE))

    # Calculate metrics
    preds_np = predictions[eval_idx].cpu().numpy()
    labels_np = data.y[eval_idx].cpu().numpy()

    roc_auc = roc_auc_score(labels_np, preds_np)
    avg_precision = average_precision_score(labels_np, preds_np)

    # Binary accuracy (threshold at 0.5)
    binary_preds = (preds_np > 0.5).astype(int)
    accuracy = (binary_preds == labels_np).mean()

    return {
        'loss': loss.item(),
        'roc_auc': roc_auc,
        'avg_precision': avg_precision,
        'accuracy': accuracy,
    }


def train_gnn():
    """Main training function"""
    print(f"Training GNN on device: {DEVICE}")
    print("=" * 80)

    # Load data
    pets_df, edges_df, breed_to_idx, temp_to_idx = load_graph_data()
    data = create_graph_dataset(pets_df, edges_df, breed_to_idx, temp_to_idx)

    # Initialize model
    n_breeds = len(breed_to_idx)
    n_temperaments = len(temp_to_idx)

    model = GraphAttentionCompatibility(
        n_breeds=n_breeds,
        n_temperaments=n_temperaments,
        embedding_dim=EMBEDDING_DIM,
        hidden_dim=HIDDEN_DIM,
        gnn_layers=GNN_LAYERS,
        attention_heads=ATTENTION_HEADS,
        dropout=DROPOUT,
    ).to(DEVICE)

    print(f"\nModel architecture:")
    print(f"  Breeds: {n_breeds}")
    print(f"  Temperaments: {n_temperaments}")
    print(f"  Embedding dim: {EMBEDDING_DIM}")
    print(f"  Hidden dim: {HIDDEN_DIM}")
    print(f"  GNN layers: {GNN_LAYERS}")
    print(f"  Attention heads: {ATTENTION_HEADS}")
    print(f"  Dropout: {DROPOUT}")
    print(f"\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}")
    print("=" * 80)

    # Optimizer and loss
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    criterion = nn.BCELoss()

    # Learning rate scheduler
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5
    )

    # Training loop
    best_val_loss = float('inf')
    patience_counter = 0
    train_history = []

    print("\nStarting training...")
    print("=" * 80)

    for epoch in range(NUM_EPOCHS):
        # Train
        train_loss = train_epoch(model, data, optimizer, criterion, data.train_idx)

        # Validate
        val_metrics = evaluate(model, data, criterion, data.val_idx, 'val')

        # Update learning rate
        scheduler.step(val_metrics['loss'])

        # Record history
        train_history.append({
            'epoch': epoch + 1,
            'train_loss': train_loss,
            'val_loss': val_metrics['loss'],
            'val_roc_auc': val_metrics['roc_auc'],
            'val_avg_precision': val_metrics['avg_precision'],
            'val_accuracy': val_metrics['accuracy'],
            'lr': optimizer.param_groups[0]['lr'],
        })

        # Print progress
        if (epoch + 1) % 5 == 0:
            print(f"Epoch {epoch + 1}/{NUM_EPOCHS}")
            print(f"  Train Loss: {train_loss:.4f}")
            print(f"  Val Loss: {val_metrics['loss']:.4f}")
            print(f"  Val ROC-AUC: {val_metrics['roc_auc']:.4f}")
            print(f"  Val Avg Precision: {val_metrics['avg_precision']:.4f}")
            print(f"  Val Accuracy: {val_metrics['accuracy']:.4f}")
            print(f"  LR: {optimizer.param_groups[0]['lr']:.6f}")
            print()

        # Early stopping
        if val_metrics['loss'] < best_val_loss - MIN_DELTA:
            best_val_loss = val_metrics['loss']
            patience_counter = 0

            # Save best model
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_metrics['loss'],
                'val_roc_auc': val_metrics['roc_auc'],
                'config': {
                    'n_breeds': n_breeds,
                    'n_temperaments': n_temperaments,
                    'embedding_dim': EMBEDDING_DIM,
                    'hidden_dim': HIDDEN_DIM,
                    'gnn_layers': GNN_LAYERS,
                    'attention_heads': ATTENTION_HEADS,
                    'dropout': DROPOUT,
                }
            }, MODEL_DIR / 'gnn_compatibility_best.pt')

            print(f"âœ“ Saved best model (epoch {epoch + 1}, val_loss: {val_metrics['loss']:.4f})")
        else:
            patience_counter += 1

        if patience_counter >= PATIENCE:
            print(f"\nEarly stopping triggered after {epoch + 1} epochs")
            break

    print("\n" + "=" * 80)
    print("Training completed!")

    # Load best model and evaluate on test set
    print("\nLoading best model for final evaluation...")
    checkpoint = torch.load(MODEL_DIR / 'gnn_compatibility_best.pt')
    model.load_state_dict(checkpoint['model_state_dict'])

    test_metrics = evaluate(model, data, criterion, data.test_idx, 'test')

    print("\nFinal Test Set Performance:")
    print(f"  Test Loss: {test_metrics['loss']:.4f}")
    print(f"  Test ROC-AUC: {test_metrics['roc_auc']:.4f}")
    print(f"  Test Avg Precision: {test_metrics['avg_precision']:.4f}")
    print(f"  Test Accuracy: {test_metrics['accuracy']:.4f}")

    # Save training history
    history_df = pd.DataFrame(train_history)
    history_df.to_csv(MODEL_DIR / 'gnn_training_history.csv', index=False)

    # Save final metrics
    final_metrics = {
        'train_epochs': len(train_history),
        'best_val_loss': best_val_loss,
        'test_metrics': test_metrics,
        'timestamp': datetime.now().isoformat(),
    }

    with open(MODEL_DIR / 'gnn_final_metrics.json', 'w') as f:
        json.dump(final_metrics, f, indent=2)

    print("\n" + "=" * 80)
    print("Training artifacts saved:")
    print(f"  Model: {MODEL_DIR / 'gnn_compatibility_best.pt'}")
    print(f"  Mappings: {MODEL_DIR / 'feature_mappings.json'}")
    print(f"  History: {MODEL_DIR / 'gnn_training_history.csv'}")
    print(f"  Metrics: {MODEL_DIR / 'gnn_final_metrics.json'}")
    print("=" * 80)


if __name__ == '__main__':
    train_gnn()
