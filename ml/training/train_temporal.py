"""
Training script for Temporal Transformer activity prediction model
Uses the temporal activity sequences generated by generate_temporal_data.py
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import json
from sklearn.model_selection import train_test_split

from models.temporal_transformer import TemporalActivityTransformer

# Configuration
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
DATA_DIR = Path('ml/data')
MODEL_DIR = Path('ml/models/saved')
MODEL_DIR.mkdir(parents=True, exist_ok=True)

# Hyperparameters
BATCH_SIZE = 32
NUM_EPOCHS = 80
LEARNING_RATE = 0.0001
WEIGHT_DECAY = 1e-5
PATIENCE = 12
MIN_DELTA = 0.001

# Model architecture
D_MODEL = 256
NUM_LAYERS = 6
NUM_HEADS = 8
D_FF = 1024
MAX_SEQ_LEN = 100
DROPOUT = 0.1

# Task weights for multi-task learning
ACTIVITY_WEIGHT = 1.0
ENERGY_WEIGHT = 0.5
DURATION_WEIGHT = 0.3
TIME_WEIGHT = 0.3


class ActivitySequenceDataset(Dataset):
    """Dataset for activity sequences"""

    def __init__(self, sequences_df, activities_df, max_seq_len=100):
        self.sequences_df = sequences_df
        self.activities_df = activities_df
        self.max_seq_len = max_seq_len

        # Create activity type mapping
        self.activity_types = sorted(activities_df['activity_type'].unique())
        self.activity_to_idx = {act: idx for idx, act in enumerate(self.activity_types)}
        self.num_activity_types = len(self.activity_types)

        # Energy level mapping
        self.energy_to_idx = {'low': 0, 'medium': 1, 'high': 2}

    def __len__(self):
        return len(self.sequences_df)

    def __getitem__(self, idx):
        sequence_row = self.sequences_df.iloc[idx]

        # Get activities for this sequence
        activity_ids = eval(sequence_row['activity_ids'])  # Convert string to list
        sequence_activities = self.activities_df[
            self.activities_df['activity_id'].isin(activity_ids)
        ].sort_values('timestamp')

        # Prepare sequence data
        seq_len = min(len(sequence_activities), self.max_seq_len)

        # Activity types
        activity_types = np.zeros(self.max_seq_len, dtype=np.long)
        # Energy levels (as input)
        energy_input = np.zeros(self.max_seq_len, dtype=np.long)
        # Duration
        durations = np.zeros(self.max_seq_len, dtype=np.float32)
        # Time of day (hour)
        times = np.zeros(self.max_seq_len, dtype=np.long)
        # Mask for padding
        mask = np.zeros(self.max_seq_len, dtype=np.bool_)

        for i, (_, act) in enumerate(sequence_activities.head(self.max_seq_len).iterrows()):
            activity_types[i] = self.activity_to_idx[act['activity_type']]
            energy_input[i] = self.energy_to_idx[act['energy_level']]
            durations[i] = act['duration_minutes'] / 120.0  # Normalize by 2 hours
            # Extract hour from timestamp
            hour = pd.to_datetime(act['timestamp']).hour
            times[i] = hour
            mask[i] = True

        # Target: predict next activity, energy, duration, optimal time
        if seq_len < self.max_seq_len:
            # For sequences shorter than max, predict next activity
            target_activity = activity_types[seq_len - 1]  # Last activity as target
            target_energy = energy_input[seq_len - 1]
            target_duration = durations[seq_len - 1]
            target_time = times[seq_len - 1]
        else:
            # For full sequences, predict continuation
            target_activity = activity_types[-1]
            target_energy = energy_input[-1]
            target_duration = durations[-1]
            target_time = times[-1]

        return {
            'activity_types': torch.tensor(activity_types, dtype=torch.long),
            'energy_input': torch.tensor(energy_input, dtype=torch.long),
            'durations': torch.tensor(durations, dtype=torch.float),
            'times': torch.tensor(times, dtype=torch.long),
            'mask': torch.tensor(mask, dtype=torch.bool),
            'target_activity': torch.tensor(target_activity, dtype=torch.long),
            'target_energy': torch.tensor(target_energy, dtype=torch.long),
            'target_duration': torch.tensor(target_duration, dtype=torch.float),
            'target_time': torch.tensor(target_time, dtype=torch.long),
            'seq_len': seq_len,
        }


def load_temporal_data():
    """Load the temporal activity data"""
    print("Loading temporal data...")

    activities_df = pd.read_csv(DATA_DIR / 'temporal_activities.csv')
    sequences_df = pd.read_csv(DATA_DIR / 'temporal_sequences.csv')

    print(f"Loaded {len(activities_df)} activities and {len(sequences_df)} sequences")

    return activities_df, sequences_df


def create_dataloaders(activities_df, sequences_df):
    """Create train/val/test dataloaders"""
    print("Creating dataloaders...")

    # Split sequences
    train_seq, temp_seq = train_test_split(sequences_df, test_size=0.3, random_state=42)
    val_seq, test_seq = train_test_split(temp_seq, test_size=0.5, random_state=42)

    # Create datasets
    train_dataset = ActivitySequenceDataset(train_seq, activities_df, MAX_SEQ_LEN)
    val_dataset = ActivitySequenceDataset(val_seq, activities_df, MAX_SEQ_LEN)
    test_dataset = ActivitySequenceDataset(test_seq, activities_df, MAX_SEQ_LEN)

    # Create dataloaders
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)

    print(f"Train sequences: {len(train_dataset)}")
    print(f"Val sequences: {len(val_dataset)}")
    print(f"Test sequences: {len(test_dataset)}")

    # Save mappings (convert all numpy types to Python types for JSON)
    mappings = {
        'activity_to_idx': {str(k): int(v) for k, v in train_dataset.activity_to_idx.items()},
        'idx_to_activity': {str(v): str(k) for k, v in train_dataset.activity_to_idx.items()},
        'energy_to_idx': {str(k): int(v) for k, v in train_dataset.energy_to_idx.items()},
        'num_activity_types': int(train_dataset.num_activity_types),
    }

    with open(MODEL_DIR / 'temporal_mappings.json', 'w') as f:
        json.dump(mappings, f, indent=2)

    return train_loader, val_loader, test_loader, train_dataset.num_activity_types


def train_epoch(model, dataloader, optimizer, criterion_activity, criterion_energy,
                criterion_duration, criterion_time):
    """Train for one epoch"""
    model.train()
    total_loss = 0
    activity_loss_sum = 0
    energy_loss_sum = 0
    duration_loss_sum = 0
    time_loss_sum = 0
    num_batches = 0

    for batch in dataloader:
        optimizer.zero_grad()

        # Move to device
        activity_types = batch['activity_types'].to(DEVICE)
        energy_input = batch['energy_input'].to(DEVICE)
        durations = batch['durations'].to(DEVICE)
        times = batch['times'].to(DEVICE)
        mask = batch['mask'].to(DEVICE)

        target_activity = batch['target_activity'].to(DEVICE)
        target_energy = batch['target_energy'].to(DEVICE)
        target_duration = batch['target_duration'].to(DEVICE)
        target_time = batch['target_time'].to(DEVICE)

        # Forward pass
        pred_activity, pred_energy, pred_duration, pred_time = model(
            activity_types, energy_input, durations, times, mask
        )

        # Calculate losses
        loss_activity = criterion_activity(pred_activity, target_activity)
        loss_energy = criterion_energy(pred_energy, target_energy)
        loss_duration = criterion_duration(pred_duration.squeeze(), target_duration)
        loss_time = criterion_time(pred_time, target_time)

        # Combined loss
        loss = (ACTIVITY_WEIGHT * loss_activity +
                ENERGY_WEIGHT * loss_energy +
                DURATION_WEIGHT * loss_duration +
                TIME_WEIGHT * loss_time)

        # Backward pass
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += loss.item()
        activity_loss_sum += loss_activity.item()
        energy_loss_sum += loss_energy.item()
        duration_loss_sum += loss_duration.item()
        time_loss_sum += loss_time.item()
        num_batches += 1

    return {
        'total_loss': total_loss / num_batches,
        'activity_loss': activity_loss_sum / num_batches,
        'energy_loss': energy_loss_sum / num_batches,
        'duration_loss': duration_loss_sum / num_batches,
        'time_loss': time_loss_sum / num_batches,
    }


@torch.no_grad()
def evaluate(model, dataloader, criterion_activity, criterion_energy,
             criterion_duration, criterion_time):
    """Evaluate the model"""
    model.eval()
    total_loss = 0
    activity_loss_sum = 0
    energy_loss_sum = 0
    duration_loss_sum = 0
    time_loss_sum = 0
    num_batches = 0

    activity_correct = 0
    energy_correct = 0
    time_correct = 0
    total_samples = 0

    for batch in dataloader:
        # Move to device
        activity_types = batch['activity_types'].to(DEVICE)
        energy_input = batch['energy_input'].to(DEVICE)
        durations = batch['durations'].to(DEVICE)
        times = batch['times'].to(DEVICE)
        mask = batch['mask'].to(DEVICE)

        target_activity = batch['target_activity'].to(DEVICE)
        target_energy = batch['target_energy'].to(DEVICE)
        target_duration = batch['target_duration'].to(DEVICE)
        target_time = batch['target_time'].to(DEVICE)

        # Forward pass
        pred_activity, pred_energy, pred_duration, pred_time = model(
            activity_types, energy_input, durations, times, mask
        )

        # Calculate losses
        loss_activity = criterion_activity(pred_activity, target_activity)
        loss_energy = criterion_energy(pred_energy, target_energy)
        loss_duration = criterion_duration(pred_duration.squeeze(), target_duration)
        loss_time = criterion_time(pred_time, target_time)

        # Combined loss
        loss = (ACTIVITY_WEIGHT * loss_activity +
                ENERGY_WEIGHT * loss_energy +
                DURATION_WEIGHT * loss_duration +
                TIME_WEIGHT * loss_time)

        total_loss += loss.item()
        activity_loss_sum += loss_activity.item()
        energy_loss_sum += loss_energy.item()
        duration_loss_sum += loss_duration.item()
        time_loss_sum += loss_time.item()
        num_batches += 1

        # Calculate accuracies
        activity_pred = pred_activity.argmax(dim=1)
        energy_pred = pred_energy.argmax(dim=1)
        time_pred = pred_time.argmax(dim=1)

        activity_correct += (activity_pred == target_activity).sum().item()
        energy_correct += (energy_pred == target_energy).sum().item()
        time_correct += (time_pred == target_time).sum().item()
        total_samples += len(target_activity)

    return {
        'total_loss': total_loss / num_batches,
        'activity_loss': activity_loss_sum / num_batches,
        'energy_loss': energy_loss_sum / num_batches,
        'duration_loss': duration_loss_sum / num_batches,
        'time_loss': time_loss_sum / num_batches,
        'activity_accuracy': activity_correct / total_samples,
        'energy_accuracy': energy_correct / total_samples,
        'time_accuracy': time_correct / total_samples,
    }


def train_temporal():
    """Main training function"""
    print(f"Training Temporal Transformer on device: {DEVICE}")
    print("=" * 80)

    # Load data
    activities_df, sequences_df = load_temporal_data()
    train_loader, val_loader, test_loader, num_activity_types = create_dataloaders(
        activities_df, sequences_df
    )

    # Initialize model
    model = TemporalActivityTransformer(
        d_model=D_MODEL,
        num_layers=NUM_LAYERS,
        num_heads=NUM_HEADS,
        d_ff=D_FF,
        max_seq_len=MAX_SEQ_LEN,
        num_activity_types=num_activity_types,
        dropout=DROPOUT,
    ).to(DEVICE)

    print(f"\nModel architecture:")
    print(f"  D_model: {D_MODEL}")
    print(f"  Num layers: {NUM_LAYERS}")
    print(f"  Num heads: {NUM_HEADS}")
    print(f"  D_ff: {D_FF}")
    print(f"  Max seq len: {MAX_SEQ_LEN}")
    print(f"  Activity types: {num_activity_types}")
    print(f"  Dropout: {DROPOUT}")
    print(f"\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}")
    print("=" * 80)

    # Optimizer and loss functions
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)

    criterion_activity = nn.CrossEntropyLoss()
    criterion_energy = nn.CrossEntropyLoss()
    criterion_duration = nn.MSELoss()
    criterion_time = nn.CrossEntropyLoss()

    # Learning rate scheduler
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5
    )

    # Training loop
    best_val_loss = float('inf')
    patience_counter = 0
    train_history = []

    print("\nStarting training...")
    print("=" * 80)

    for epoch in range(NUM_EPOCHS):
        # Train
        train_metrics = train_epoch(
            model, train_loader, optimizer,
            criterion_activity, criterion_energy, criterion_duration, criterion_time
        )

        # Validate
        val_metrics = evaluate(
            model, val_loader,
            criterion_activity, criterion_energy, criterion_duration, criterion_time
        )

        # Update learning rate
        scheduler.step(val_metrics['total_loss'])

        # Record history
        train_history.append({
            'epoch': epoch + 1,
            'train_loss': train_metrics['total_loss'],
            'val_loss': val_metrics['total_loss'],
            'val_activity_acc': val_metrics['activity_accuracy'],
            'val_energy_acc': val_metrics['energy_accuracy'],
            'val_time_acc': val_metrics['time_accuracy'],
            'lr': optimizer.param_groups[0]['lr'],
        })

        # Print progress
        if (epoch + 1) % 5 == 0:
            print(f"Epoch {epoch + 1}/{NUM_EPOCHS}")
            print(f"  Train Loss: {train_metrics['total_loss']:.4f}")
            print(f"  Val Loss: {val_metrics['total_loss']:.4f}")
            print(f"  Val Activity Acc: {val_metrics['activity_accuracy']:.4f}")
            print(f"  Val Energy Acc: {val_metrics['energy_accuracy']:.4f}")
            print(f"  Val Time Acc: {val_metrics['time_accuracy']:.4f}")
            print(f"  LR: {optimizer.param_groups[0]['lr']:.6f}")
            print()

        # Early stopping
        if val_metrics['total_loss'] < best_val_loss - MIN_DELTA:
            best_val_loss = val_metrics['total_loss']
            patience_counter = 0

            # Save best model
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_metrics['total_loss'],
                'val_accuracies': {
                    'activity': val_metrics['activity_accuracy'],
                    'energy': val_metrics['energy_accuracy'],
                    'time': val_metrics['time_accuracy'],
                },
                'config': {
                    'd_model': D_MODEL,
                    'num_layers': NUM_LAYERS,
                    'num_heads': NUM_HEADS,
                    'd_ff': D_FF,
                    'max_seq_len': MAX_SEQ_LEN,
                    'num_activity_types': num_activity_types,
                    'dropout': DROPOUT,
                }
            }, MODEL_DIR / 'temporal_transformer_best.pt')

            print(f"âœ“ Saved best model (epoch {epoch + 1}, val_loss: {val_metrics['total_loss']:.4f})")
        else:
            patience_counter += 1

        if patience_counter >= PATIENCE:
            print(f"\nEarly stopping triggered after {epoch + 1} epochs")
            break

    print("\n" + "=" * 80)
    print("Training completed!")

    # Load best model and evaluate on test set
    print("\nLoading best model for final evaluation...")
    checkpoint = torch.load(MODEL_DIR / 'temporal_transformer_best.pt')
    model.load_state_dict(checkpoint['model_state_dict'])

    test_metrics = evaluate(
        model, test_loader,
        criterion_activity, criterion_energy, criterion_duration, criterion_time
    )

    print("\nFinal Test Set Performance:")
    print(f"  Test Loss: {test_metrics['total_loss']:.4f}")
    print(f"  Test Activity Accuracy: {test_metrics['activity_accuracy']:.4f}")
    print(f"  Test Energy Accuracy: {test_metrics['energy_accuracy']:.4f}")
    print(f"  Test Time Accuracy: {test_metrics['time_accuracy']:.4f}")

    # Save training history
    history_df = pd.DataFrame(train_history)
    history_df.to_csv(MODEL_DIR / 'temporal_training_history.csv', index=False)

    # Save final metrics
    final_metrics = {
        'train_epochs': len(train_history),
        'best_val_loss': best_val_loss,
        'test_metrics': {k: float(v) for k, v in test_metrics.items()},
        'timestamp': datetime.now().isoformat(),
    }

    with open(MODEL_DIR / 'temporal_final_metrics.json', 'w') as f:
        json.dump(final_metrics, f, indent=2)

    print("\n" + "=" * 80)
    print("Training artifacts saved:")
    print(f"  Model: {MODEL_DIR / 'temporal_transformer_best.pt'}")
    print(f"  Mappings: {MODEL_DIR / 'temporal_mappings.json'}")
    print(f"  History: {MODEL_DIR / 'temporal_training_history.csv'}")
    print(f"  Metrics: {MODEL_DIR / 'temporal_final_metrics.json'}")
    print("=" * 80)


if __name__ == '__main__':
    train_temporal()
